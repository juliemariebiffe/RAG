import os
import tempfile

import streamlit as st
import pandas as pd

from rag import my_langchain
from rag.my_llamaindex import MyLlamaIndex  # ‚úÖ Nouvelle classe LlamaIndex

st.set_page_config(
    page_title="Analyse de documents",
    page_icon="üëã",
)

# Initialisation des √©tats Streamlit pour garder la trace des fichiers, framework et index
if 'stored_files' not in st.session_state:
    st.session_state['stored_files'] = []  # Liste des noms de fichiers charg√©s
if 'framework' not in st.session_state:
    st.session_state['framework'] = "langchain"  # Framework utilis√© (langchain ou llamaindex)
if 'llamaindex_instance' not in st.session_state:
    st.session_state['llamaindex_instance'] = MyLlamaIndex()  # Instance singleton LlamaIndex
if 'file_buffers' not in st.session_state:
    st.session_state['file_buffers'] = {}  # Pour stocker en m√©moire les fichiers PDF (LangChain)

llamaindex = st.session_state['llamaindex_instance']


def clear_indexes():
    """
    R√©initialise les indexes de LangChain et LlamaIndex.
    Important lors du changement de framework ou de suppression totale.
    """
    my_langchain.clear_index()
    llamaindex.clear_index()


def rebuild_langchain_index():
    """
    Reconstruit compl√®tement l'index LangChain √† partir des fichiers PDF
    stock√©s en m√©moire dans `file_buffers`.
    Utilis√© apr√®s ajout ou suppression de fichiers en mode LangChain.
    """
    my_langchain.clear_index()
    for filename, file_bytes in st.session_state['file_buffers'].items():
        # Cr√©ation d'un fichier temporaire pour que LangChain puisse le charger
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file_bytes)
            tmp_file.flush()
            # Stockage dans l'index LangChain
            my_langchain.store_pdf_file(tmp_file.name, filename)
            # Suppression du fichier temporaire imm√©diatement apr√®s l'indexation
            os.unlink(tmp_file.name)


def main():
    st.title("Analyse de documents")
    st.subheader("Chargez vos documents et posez vos questions √† l‚ÄôIA")

    # S√©lection du framework d'indexation √† utiliser
    framework = st.radio(
        "Choisissez le framework d'indexation",
        options=["langchain", "llamaindex"],
        index=0 if st.session_state['framework'] == "langchain" else 1
    )

    # Si changement de framework, on reset les √©tats, fichiers et indexes
    if framework != st.session_state['framework']:
        st.session_state['stored_files'] = []
        st.session_state['file_buffers'] = {}
        clear_indexes()
        st.session_state['framework'] = framework
        st.experimental_rerun()  # Recharge la page pour appliquer les changements

    # Upload multiple fichiers PDF
    uploaded_files = st.file_uploader(
        "D√©posez vos fichiers ici ou chargez-les",
        accept_multiple_files=True,
        type=["pdf"]
    )

    file_info = []  # Pour afficher le tableau des fichiers charg√©s

    if uploaded_files:
        for f in uploaded_files:
            size_kb = len(f.getvalue()) / 1024
            file_info.append({
                "Nom du fichier": f.name,
                "Taille (KB)": f"{size_kb:.2f}"
            })

            # Ajout uniquement si le fichier n'a pas d√©j√† √©t√© charg√©
            if f.name not in st.session_state['stored_files']:
                st.session_state['stored_files'].append(f.name)

                if framework == "langchain":
                    # Pour LangChain, on stocke le contenu en m√©moire
                    st.session_state['file_buffers'][f.name] = f.getvalue()
                else:
                    # Pour LlamaIndex, on √©crit le fichier dans un r√©pertoire temporaire
                    temp_dir = tempfile.mkdtemp()
                    temp_path = os.path.join(temp_dir, f.name)
                    with open(temp_path, "wb") as tmpf:
                        tmpf.write(f.getvalue())
                    # On indexe imm√©diatement ce fichier
                    llamaindex.store_pdf_file(temp_path, f.name)

        # Reconstruction de l'index LangChain uniquement (car stock en m√©moire)
        if framework == "langchain":
            rebuild_langchain_index()

    else:
        # Aucun fichier upload√©, affichage de la liste des fichiers d√©j√† stock√©s
        for name in st.session_state['stored_files']:
            # Taille inconnue ici car non accessible, affichage "-"
            file_info.append({"Nom du fichier": name, "Taille (KB)": "-"})

    # Gestion des fichiers supprim√©s par l'utilisateur via l'interface uploader
    current_files = {f['Nom du fichier'] for f in file_info}
    removed_files = set(st.session_state['stored_files']) - current_files
    if removed_files:
        for name in removed_files:
            st.session_state['stored_files'].remove(name)
            # Suppression du buffer en m√©moire uniquement pour LangChain
            if framework == "langchain" and name in st.session_state['file_buffers']:
                del st.session_state['file_buffers'][name]

        # Reconstruction ou nettoyage des index en fonction du framework
        if framework == "langchain":
            rebuild_langchain_index()
        else:
            # LlamaIndex ne g√®re pas la suppression individuelle proprement
            # Il faudrait envisager de reconstruire l'index complet si suppression
            pass

    # Affichage du tableau des fichiers charg√©s avec leur taille
    if file_info:
        df = pd.DataFrame(file_info)
        st.table(df)

    # Slider pour choisir le nombre de documents similaires √† r√©cup√©rer (k) - LangChain uniquement
    k = st.slider(
        "Nombre de documents similaires √† r√©cup√©rer (k)",
        min_value=1,
        max_value=20,
        value=5,
        step=1
    )

    # Choix de la langue pour la r√©ponse (utilis√© uniquement pour LangChain)
    language = st.selectbox(
        "Choisissez la langue de r√©ponse",
        options=["fran√ßais", "anglais", "espagnol", "allemand"],
        index=0
    )

    # Champ texte pour poser une question
    question = st.text_input("Votre question ici")

    # Bouton d'analyse
    if st.button("Analyser"):
        if not question:
            st.warning("Veuillez entrer une question avant d'analyser.")
        else:
            # Appel de la fonction de r√©ponse selon le framework choisi
            if framework == "langchain":
                model_response = my_langchain.answer_question(question, language, k)
            else:
                model_response = llamaindex.answer_question(question)

            # Affichage de la r√©ponse dans une zone texte
            st.text_area("R√©ponse du mod√®le", value=model_response, height=200)

            # Syst√®me simple de feedback utilisateur (1 √† 5 √©toiles)
            feedback = st.radio(
                "Que pensez-vous de la qualit√© de la r√©ponse ?",
                options=[1, 2, 3, 4, 5],
                index=4,
                format_func=lambda x: f"{x} √©toiles",
                key="user_feedback"
            )
            if feedback is not None:
                print(f"Feedback utilisateur: {feedback}")


if __name__ == "__main__":
    main()
